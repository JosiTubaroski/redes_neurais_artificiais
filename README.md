# Redes Neurais Artificiais

Vamos dividir em 4 partes para ficar bem clara: <b>origem, evolu√ß√£o hist√≥rica, como funciona</b>, e <b>por que voltaram com tanta for√ßa.</b>

### üß† 1. Origem: inspira√ß√£o biol√≥gica

As redes neurais artificiais foram inspiradas no <b>funcionamento do c√©rebro humano.</b>
Nosso c√©rebro √© formado por <b>neur√¥nios</b> que se comunicam atrav√©s de <b>sinapse</b>, transmitindo sinais el√©tricos e qu√≠micos.
A id√©ia era simples, mas revolucion√°ria:

"E se cri√°ssemos um sistema computacional que imitasse esse comportamento - aprender a partir de exemplos, assim como o c√©rebro faz?"

O promeiro modelo surgiu em <b>1943</b>, com os cientistas <b>Warren McCulloch</b> e <b>Walter Pitts</b>, que publicaram o artigo

"A Logical Calculus of Ideas Immanent in Nervous Activity".

Eles criaram um modelo matem√°tico de neur√¥nio simples: recebia entradas, aplicava um peso, somava e gerava uma sa√≠da (ativa√ß√£o).

### üß© 2. Linha do tempo hist√≥rica

- <b>1950-1960</b> ‚Üí surgem os primeiros experimentos pr√°ticos, como o <b>Perceptron</b> (Frank Rosenblatt, 1958).
  Ele aprendia a classificar padr√µes simples, mas tinha limita√ß√µes - n√£o resolvia problemas mais complexos (como o famoso XOR).

- <b>1970-1980</b> ‚Üí "inverno da IA": o entusiasmo caiu, porque as m√°quinas eram lentas e os modelos, fracos.

- <b>1986</b> ‚Üí ressurgimento com o <b>algoritmo de retropropaga√ß√£o (backpropagation)</b>, criado por <b>Rumelhart, Hinton e Williams,</b> permitindo treinar redes com v√°rias camadas.

- <b>1990-2000</b> ‚Üí as RNAs come√ßaram a ser aplicadas em reconhecimento de voz, escrita e imagem.

- <b>2010 em diante</b> ‚Üí renascimento do ‚ÄúDeep Learning‚Äù com <b>redes neurais profundas</b> (muitas camadas) e <b>grande poder computacional (GPUs).</b>
  Empresas como Google, Meta e OpenAI impulsionaram essa nova era.

### ‚öôÔ∏è 3. Como funcionam as redes neurais artificiais

De uma forma simples, uma <b>rede neural artificial</b> √© um conjunto de <b>camadas de neur√¥nios artificiais</b> (tamb√™m chamados de n√≥s ou unidades), conectados entre si.
Cada conex√£o tem um <b>peso</b>, que indica a import√¢ncia do sinal.

### Processo B√°sico:

<b>1. Entrada</b> ‚Üí os dados entram (por exemplo, uma imagem com pixels).
<b>2. Propaga√ß√£o</b> ‚Üí cada neur√¥nio multiplica o valor de entrada por um peso, soma tudo e aplica uma <b>fun√ß√£o de ativa√ß√£o</b> (como ReLU ou Sigmoid).
<b>3. Sa√≠da</b> ‚Üí a rede gera um resultado (ex: ‚Äúgato‚Äù ou ‚Äúcachorro‚Äù).
<b>4. Aprendizado (treinamento)</b> ‚Üí o sistema compara o resultado com o valor real e ajusta os pesos (via backpropagation e gradiente descendente) para errar menos na pr√≥xima vez.

Com muitas itera√ß√µes, a rede <b>aprende padr√µes complexos</b> ‚Äî por exemplo, diferenciar rostos, traduzir textos, ou prever comportamento de clientes.

### üöÄ 4. Por que voltaram com tanta for√ßa

- Aumento do <b>poder computacional</b> (GPUs e TPUs).
- <b>Big Data</b> (milhoes de exemplos para aprender).
- <b>Novos algoritimos</b> e arquiteturas (como CNNs, RNNs, Transformers).
- <b>Open Source</b> (TensorFlow, PyTorch, etc.).

Hoje, redes neurais est√£o por tr√°s de:

- Reconhecimento facial
- Tradu√ß√£o autom√°tica
- Carros aut√¥nomos
- ChatGPT (que usa uma arquitetura derivada: o <b>Transformer</b>)
